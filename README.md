# LLM Evaluation
This repository explores common evaluation methods for Large Language Models (LLMs), focusing on practical and standardized approaches to measure model performance. It aims to provide reproducible implementations, comparisons, and insights into strengths and limitations of various LLM evaluation metrics across use cases such as summarization, classification, and generation. The goal is to help practitioners select appropriate evaluation tools and integrate them into real-world model validation workflows.

# Resources
* [Evaluating LLM Systems: Metrics, Challenges, and Best Practices (Microsoft)](https://medium.com/data-science-at-microsoft/evaluating-llm-systems-metrics-challenges-and-best-practices-664ac25be7e5)
* [Kolena Evaluation Metrics Documentation](https://docs.kolena.com/metrics/)
* [Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation](https://arxiv.org/abs/2503.19092)