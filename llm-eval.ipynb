{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f0a6a3f27bb3181",
   "metadata": {},
   "source": [
    "### [BLEU](https://huggingface.co/spaces/evaluate-metric/bleu)\n",
    "* BLEU (Bilingual Evaluation Understudy) is a metric originally designed for evaluating machine translation models by comparing generated text to reference translations. It measures the overlap of n-grams between the generated output and the reference texts while applying a brevity penalty to discourage excessively short translations. BLEU ranges from 0 to 1, where a higher score indicates better similarity to the reference.\n",
    "\n",
    "* n-gram:\n",
    "  - max_order (int): Maximum n-gram order to use when computing BLEU score. Defaults to 4.\n",
    "\n",
    "#### Tutorial\n",
    "* [What is BLEU metric?](https://www.youtube.com/watch?v=M05L1DhFqcw)\n",
    "\n",
    "\n",
    "* [BLEU examples with step-by-step calculation](https://docs.kolena.com/metrics/bleu/)\n",
    "\n",
    "* ```json\n",
    "   {\n",
    "     'bleu': 0.40548013303822666,   // Geometric mean of n-gram (4) precisions\n",
    "     'precisions': [0.6666666666666666, 0.4, 0.25], # 1-gram, 2-gram, 3-gram, 4-gram\n",
    "     'brevity_penalty': 1.0,\n",
    "     'length_ratio': 2.0,\n",
    "     'translation_length': 6,\n",
    "     'reference_length': 3\n",
    "   }\n",
    "  ```\n",
    "#### Limitations of BLEU\n",
    "* BLEU compares overlap in tokens from the predictions and references, instead of comparing meaning. This can lead to discrepancies between BLEU scores and human ratings.\n",
    "\n",
    "* Doesn't incorporate sentence structure.\n",
    "  - Shorter predicted translations achieve higher scores than longer ones, simply due to how the score is calculated. A brevity penalty is introduced to attempt to counteract this.\n",
    "\n",
    "* BLEU scores are not comparable across different datasets, nor are they comparable across different languages.\n",
    "\n",
    "* BLEU scores can vary greatly depending on which parameters are used to generate the scores, especially when different tokenization and normalization techniques are used. It is therefore not possible to compare BLEU scores generated using different parameters, or when these parameters are unknown. For more discussion around this topic, see the following [issue](https://github.com/huggingface/datasets/issues/137).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d215b868e6dce35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T20:04:41.597482Z",
     "start_time": "2025-03-28T20:04:40.837873Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sglee/anaconda3/envs/llm-eval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.3086066999241838,\n",
       " 'precisions': [0.5714285714285714, 0.16666666666666666],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 1.4,\n",
       " 'translation_length': 7,\n",
       " 'reference_length': 5}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Generated text from LLM\n",
    "predictions = [\"I really loved reading the Hunger Games\"]\n",
    "\n",
    "# Reference texts (ground truth responses)\n",
    "references = [[\"I enjoyed reading Hunger Games\"]]\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "bleu.compute(predictions=predictions, references=references, tokenizer=word_tokenize, max_order=2)  # Override default tokenizer and n-gram (Default is 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7453b5b4f2461ff3",
   "metadata": {},
   "source": [
    "#### Sacrebleu\n",
    "* Recommended to address tokenization limitations with bleu.\n",
    "\n",
    "* ```json\n",
    "  {'score': 32.46679154750991,  // Ranges between 0 and 100 with 100 being identical.\n",
    "   'counts': [4, 2, 1, 0],\n",
    "   'totals': [6, 5, 4, 3],\n",
    "   'precisions': [66.66666666666667, 40.0, 25.0, 16.666666666666668],\n",
    "   'bp': 1.0,\n",
    "   'sys_len': 6,\n",
    "   'ref_len': 5}\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bb589d0eed7ffe9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T20:04:22.349350Z",
     "start_time": "2025-03-28T20:04:21.774333Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 15.619699684601283,\n",
       " 'counts': [4, 1, 0, 0],\n",
       " 'totals': [7, 6, 5, 4],\n",
       " 'precisions': [57.142857142857146, 16.666666666666668, 10.0, 6.25],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 7,\n",
       " 'ref_len': 5}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# Generated text from LLM\n",
    "predictions = [\"I really loved reading the Hunger Games\"]\n",
    "\n",
    "# Reference texts (ground truth responses)\n",
    "references = [[\"I enjoyed reading Hunger Games\"]]\n",
    "\n",
    "bleu = evaluate.load(\"sacrebleu\")\n",
    "bleu.compute(predictions=predictions, references=references)  # No need to pass in tokenizer. sacrebleu has its own tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5b649833bfc8b6",
   "metadata": {},
   "source": [
    "### [METEOR](https://huggingface.co/spaces/evaluate-metric/meteor)\n",
    "* METEOR (Metric for Evaluation of Translation with Explicit ORdering)\n",
    "METEOR is an NLP evaluation metric designed to improve upon BLEU by incorporating semantic matching, stemming, synonyms, and recall in addition to n-gram precision. Unlike BLEU, which focuses purely on n-gram overlap, METEOR allows for flexible matching by considering variations of words.\n",
    "\n",
    "#### Tutorial\n",
    "* [METEOR: A metric for Machine Translation](https://www.youtube.com/watch?v=FqQbrlEh_b0)\n",
    "\n",
    "* [METEOR examples with step-by-step calculation](https://docs.kolena.com/metrics/meteor/)\n",
    "\n",
    "#### Limitations and Biases\n",
    "Although METEOR was created to address some of the major limitations of BLEU, it still comes with its own limitations.\n",
    "\n",
    "* METEOR can fail on context. If we have two sentences \"I am a big fan of Taylor Swift\" (Reference) and \"Fan of Taylor Swift I am big\" (Candidate), METEOR would yield a good score. However, the candidate sentence makes little sense and intuitively shouldn't be given a good score. This is a limitation with all n-gram metrics, and not specific to METEOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "927940b38f33ddeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T20:04:25.915226Z",
     "start_time": "2025-03-28T20:04:22.556912Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/sglee/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/sglee/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/sglee/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# Generated text from LLM\n",
    "predictions = [\"I really loved reading the Hunger Games\"]\n",
    "\n",
    "# Reference texts (ground truth responses)\n",
    "references = [[\"I enjoyed reading Hunger Games\"]]\n",
    "\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "results = meteor.compute(predictions=predictions, references=references)  # No need to pass in tokenizer. sacrebleu has its own tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2453fb23437ae5b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T20:04:25.924765Z",
     "start_time": "2025-03-28T20:04:25.921646Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meteor': 0.8576923076923078}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e8a50d41381b11",
   "metadata": {},
   "source": [
    "### [ROUGE](https://huggingface.co/spaces/evaluate-metric/rouge)\n",
    "* ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics designed to evaluate text summarization and text generation models by comparing the overlap of n-grams, word sequences, and longest common subsequences between the generated text and reference texts. Unlike BLEU, which is precision-focused, ROUGE emphasizes recall, making it more suitable for tasks where capturing the full meaning is important.\n",
    "\n",
    "* Note that ROUGE is case-insensitive, meaning that upper case letters are treated the same way as lower case letters.\n",
    "\n",
    "* Valid rouge_types:\n",
    "  - \"rouge1\": unigram (1-gram) based scoring\n",
    "  - \"rouge2\": bigram (2-gram) based scoring\n",
    "  - \"rougeL\": Longest common subsequence based scoring.\n",
    "  - \"rougeLSum\": splits text using \"\\n\"\n",
    "\n",
    "#### Tutorial\n",
    "* [What is ROUGE metric?](https://www.youtube.com/watch?v=TMshhnrEXlg)\n",
    "\n",
    "* [ROUGE-N examples with step-by-step calculation](https://docs.kolena.com/metrics/rouge-n/)\n",
    "\n",
    "#### Limitations and Biases\n",
    "ROUGE-N, like any other n-gram based metric, suffers from the following limitations:\n",
    "\n",
    "* Unlike BERTScore, ROUGE-N is not able to consider order, context, or semantics when calculating a score. Since it only relies on overlapping n-grams, it can not tell when a synonym is being used or if the placement of two matching n-grams have any meaning on the overall sentence. As a result, the metric may not be a perfect representation of the quality of the text, but rather the \"likeness\" of the n-grams in two sentences. Take for example, the ROUGE-2 score of \"This is an example of text\" and \"Is an example of text this\". Both ROUGE-1 and ROUGE-2 would give this a (nearly) perfect score, but the second sentence makes absolutely no sense!\n",
    "\n",
    "* ROUGE-N can not capture global coherence. Given a long paragraph, realistically, having too large of a value for N would not return a meaningful score for two sentences, but having a reasonable number like N = 3 wouldn't be able to capture the flow of the text. The score might yield good results, but the entire paragraph might not flow smoothly at all. This is a weakness of n-gram based metrics, as they are limited to short context windows.\n",
    "\n",
    "* See [Schluter (2017)](https://aclanthology.org/E17-2007/) for an in-depth discussion of many of ROUGE’s limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e97a236a7b375acf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T20:04:26.586500Z",
     "start_time": "2025-03-28T20:04:25.942116Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.6666666666666666,\n",
       " 'rouge2': 0.2,\n",
       " 'rougeL': 0.6666666666666666,\n",
       " 'rougeLsum': 0.6666666666666666}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# Generated text from LLM\n",
    "predictions = [\"I really loved reading the Hunger Games\"]\n",
    "\n",
    "# Reference texts (ground truth responses)\n",
    "references = [[\"I enjoyed reading Hunger Games\"]]\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "rouge.compute(predictions=predictions, references=references)   # returns a dictionary of f1 scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5301683e8281362",
   "metadata": {},
   "source": [
    "### BERTScore\n",
    "* BERTScore is an NLP evaluation metric that uses deep contextual embeddings (from models like BERT) to compare generated text with reference text. Unlike BLEU, ROUGE, or METEOR, which rely on n-gram overlap, BERTScore captures semantic similarity by computing token embeddings and measuring their alignment.\n",
    "\n",
    "#### Tutorial\n",
    "* [BERTScore For LLM Evaluation](https://www.comet.com/site/blog/bertscore-for-llm-evaluation/)\n",
    "\n",
    "* [BERTScore examples with step-by-step calculation](https://docs.kolena.com/metrics/bertscore/)\n",
    "\n",
    "#### Paper\n",
    "* [BERTScore: Evaluating Text Generation with BERT](https://iclr.cc/virtual_2020/poster_SkeHuCVFDr.html)\n",
    "\n",
    "#### Limitations and Biases\n",
    "BERTScore, originally designed to be a replacement to the BLEU score and other n-gram similarity metrics, is a powerful metric that closely aligns with human judgement. However, it comes with limitations.\n",
    "\n",
    "* BERTScore is computationally expensive. The default model (roberta-large) used to calculate BERTScore requires 1.4GB of weights to be stored, and requires a forward pass through the model in order to calculate the score. This may be computationally expensive for large datasets, compared to n-gram-based metrics which are straightforward and easy to compute. However, smaller distilled models like distilbert-base-uncased can be used instead to reduce the computational cost, at the cost of reduced alignment with human judgement.\n",
    "\n",
    "* BERTScore is calculated using a black-box pretrained model. The score can not be easily explained, as the embedding space of BERT is a dense and complex representation that is only understood by the model. Though the metric provides a numerical score, it does not explain how or why the particular score was assigned. In contrast, n-gram-based metrics can easily be calculated by inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "862b6730acf7a997",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T20:04:28.972625Z",
     "start_time": "2025-03-28T20:04:26.593175Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': [0.9289501309394836],\n",
       " 'recall': [0.9711634516716003],\n",
       " 'f1': [0.949587881565094],\n",
       " 'hashcode': 'distilbert-base-uncased_L5_no-idf_version=0.3.12(hug_trans=4.50.3)'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# Generated text from LLM\n",
    "predictions = [\"I really loved reading the Hunger Games\"]\n",
    "\n",
    "# Reference texts (ground truth responses)\n",
    "references = [[\"I enjoyed reading Hunger Games\"]]\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cuda:1\")    # if CUDA is available\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "bertscore.compute(predictions=predictions, references=references, model_type=\"distilbert-base-uncased\", device='mps')   # mps is Mac M family GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c07dc10b990695a",
   "metadata": {},
   "source": [
    "### **Comparison: BERTScore vs. BLEU vs. ROUGE vs. METEOR**\n",
    "\n",
    "#### **1. Overview**\n",
    "| **Metric**  | **Focus** | **Best For** | **Key Features** |\n",
    "|------------|----------|-------------|------------------|\n",
    "| **BLEU**   | Precision | Machine Translation, Structured Outputs | Measures **n-gram overlap**, penalizes extra words, lacks recall. |\n",
    "| **ROUGE**  | Recall   | Summarization, Keyphrase Extraction | Measures **n-gram & longest common subsequence (LCS) overlap**, prioritizes recall. |\n",
    "| **METEOR** | Precision + Recall | Machine Translation, LLM Evaluation, Paraphrase Detection | Uses **stemming, synonyms, paraphrases**, recall-weighted F-score, penalizes word order errors. |\n",
    "| **BERTScore** | Semantic Similarity | Text Generation, Summarization, Paraphrasing, LLM Evaluation | Uses **pre-trained embeddings (BERT/RoBERTa)** for **meaning-based** comparison. |\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. How They Work**\n",
    "| **Feature**  | **BLEU** | **ROUGE** | **METEOR** | **BERTScore** |\n",
    "|------------|--------|--------|--------|------------|\n",
    "| **N-gram Matching** | ✅ Yes (1-4 grams) | ✅ Yes (1-gram, 2-gram, etc.) | ✅ Yes (flexible n-gram) | ❌ No |\n",
    "| **Semantic Matching (Stemming, Synonyms, Paraphrases)** | ❌ No | ❌ No | ✅ Yes | ✅ Yes (via contextual embeddings) |\n",
    "| **Precision-Based** | ✅ Yes | ❌ No | ✅ Yes | ✅ Yes |\n",
    "| **Recall-Based** | ❌ No | ✅ Yes | ✅ Yes (weighted) | ✅ Yes |\n",
    "| **Word Order Sensitivity** | ❌ No | ⚠️ Partial (only in ROUGE-L) | ✅ Yes | ✅ Captures reordering effects |\n",
    "| **Contextual Understanding** | ❌ No | ❌ No | ⚠️ Limited | ✅ Yes (learns from BERT-like models) |\n",
    "| **Sentence-Level Evaluation** | ❌ No (works best at corpus level) | ⚠️ Some variants (ROUGE-L) | ✅ Yes | ✅ Yes |\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. When to Use Each Metric**\n",
    "| **Use Case** | **Best Metric** | **Why?** |\n",
    "|-------------|--------------|--------|\n",
    "| **Machine Translation** | METEOR or BERTScore | METEOR handles synonyms; BERTScore captures meaning. |\n",
    "| **Summarization** | ROUGE or BERTScore | ROUGE captures recall; BERTScore understands reworded summaries. |\n",
    "| **LLM Response Evaluation** | BERTScore | BERTScore understands sentence meaning better than n-grams. |\n",
    "| **Code Generation / Structured Outputs** | BLEU | BLEU enforces **exact matches** for structured formats. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b2e20e77560db9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T20:04:29.005423Z",
     "start_time": "2025-03-28T20:04:29.001332Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
